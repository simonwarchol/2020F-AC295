{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> APCOMP 295 Advanced Practical Data Science\n",
    "## Exercise 3: Dask\n",
    "\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2020**<br/>\n",
    "**Instructors**: Pavlos Protopapas\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each assignment is graded out of 5 points.  The topic for this assignment is Dask.**\n",
    "\n",
    "**Due:** 09/29/2020 10:15 AM EDT\n",
    "\n",
    "**Submit:** We won't be re running your notebooks, please ensure output is visible in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Reflection on Exercise 2  (1 points)\n",
    "\n",
    "In exercise 2, we developed our app, tested it locally, afterward we deploy it with docker and finally with minikube and/or gcloud.  Please answer following questions in 1-3 sentences. \n",
    "\n",
    "<br/>\n",
    "\n",
    "(A) For the local code files:<br/>\n",
    "1.  Which port is `maindb.py` listening at?  <br/>\n",
    "Port 8082\n",
    "2. Which port is `task1.py` listening at? <br/>\n",
    "Port 8081\n",
    "<br/>\n",
    "\n",
    "(B) For the Docker implementation we built two images: `webapp:db` and `task1:frontend` \n",
    "<br>\n",
    "1. For `webapp:db` which port is exposed by the container? Are you binding any port from host (i.e your laptop) to container port ? <br/>\n",
    "<br/>  \n",
    "We are exposing port 8082 in the container for the database. This is not bound to any host port, as only the frontend will engage with it directly.\n",
    "2. For `task1:frontend` which port is exposed by the container? Are you binding any port from host (i.e your laptop) to container port ?  <br/>\n",
    "<br/> \n",
    "we are exposing port 8081 on the container. This is being bound to port 5000 on the host\n",
    "3.  There's an environment variable in `Docker_task1frontend` named `DB_HOST`. How is that getting utilized? <br/>\n",
    "Reminder:  We used these 2 commands to run our images (or used docker-compose) - <br/> `docker run --name mywebdb -d --network appNetwork webapp:db` <br/>\n",
    "`docker run --name fe -d -p 5000:8081 -e DB_HOST=mywebdb --network appNetwork task1:frontend` <br/> \n",
    "Hint: Check `task1.py` \n",
    "<br/>\n",
    "We pass the dbhost variable to the front end, which then allows up to generate the URL for the db on the fly. This allows for some flexibility, as the code (with a minor change) allows either for a purely local db, one in another container within the local machine, or on a cloud container.\n",
    "\n",
    "\n",
    "(C) For the minikube/gcloud implementation, <br/>\n",
    "1. How is the environment variable `DB_HOST`  getting populated ? \n",
    "In this case, in the `task1_deployment_k8s.yaml` yml file we set the environment variable as coming from the webappdb-configmap under the key database_host. Then in the webapp_configmap.yaml file we set the value as the service name for the db .\n",
    "2. How are we able to access our front end via browser ? <br/>\n",
    "We expose port 8081 on the cloud instance and are then able to hit the front end directly at that port on the VM IP.\n",
    "    \n",
    "3. What is the purpose of `webapp-db-service` in `webapp_db_deployment_k8s.yaml` ?<br/>\n",
    "   <br/>\n",
    "This simply declares the name of our database service, which we then use in the front end to generate the URL of database service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Compute Pi with a Slowly Converging Series (1 points)\n",
    "\n",
    "Leibniz published one of the oldest known series in 1676.  While this is easy to understand and derive, it converges very slowly.\n",
    "https://en.wikipedia.org/wiki/Leibniz_formula_for_%CF%80 <br/>\n",
    "$$\\frac{\\pi}{4} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} ...$$\n",
    "\n",
    "While this is a genuinely cruel way to compute the value of $\\pi$, it’s a fun opportunity to use brute force on a problem instead of thinking.\n",
    "Compute $\\pi$ using at least four billion terms in this sequence. Compare your time taken with numpy and dask.  On my mac, with numpy this took 44 seconds and with dask it took 5.7 seconds. \n",
    "\n",
    "*Hint:* Use dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi Estimation: 3.14159265258996\n",
      "Time 27.31100106239319\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "start = time.time()\n",
    "seq_max = 4000001\n",
    "num_terms = math.ceil(seq_max/2.0,)\n",
    "leibniz = lambda n: 1 / (n)\n",
    "even = np.arange(start=1, stop=2000000000, step=4)\n",
    "odd = np.arange(start=3, stop=2000000003, step=4)\n",
    "odd = odd * -1 \n",
    "even = leibniz(even)\n",
    "odd = leibniz(odd)\n",
    "pi  = 4 * (np.sum(even)+np.sum(odd)) \n",
    "print(\"Numpy Pi Estimation:\", pi)\n",
    "print(\"Numpy Time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask Pi Estimation: 3.1415926525897753\n",
      "Dask Time 3.930753231048584\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "\n",
    "start = time.time()\n",
    "leibniz = lambda n: 1 / (n)\n",
    "even = da.arange(1,2000000000, 4)\n",
    "odd = da.arange(3, 2000000003, 4)\n",
    "odd = odd * -1 \n",
    "even = leibniz(even)\n",
    "odd = leibniz(odd)\n",
    "pi  = 4 * (even.sum().compute()+odd.sum().compute()) \n",
    "print(\"Dask Pi Estimation:\", pi)\n",
    "print(\"Dask Time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.14159265258996"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Filter Parking Tickets Dataset (1 points)\n",
    "\n",
    "Please download the data set from https://www.kaggle.com/new-york-city/nyc-parking-tickets. According to the documentation for the parking tickets data set, the column called ‘Plate Type’ consists mainly of two different types: ‘PAS’ and ‘COM’, presumably for passenger and commercial vehicles, respectively. Maybe the rest are the famous parking tickets from the UN diplomats, who take advantage of diplomatic immunity not to pay their fines.\n",
    "\n",
    "Create a filtered Dask DataFrame with only the commercial plates.\n",
    "Persist it, so it is available in memory for future computations. Count the number of summonses in 2017 issued to commercial plate types. Compute them as a percentage of the total data set. \n",
    "\n",
    "*Hint*: This is easy; it is only about 5-7 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('Parking_Violations_Issued_-_Fiscal_Year_2017.csv')\n",
    "com_df = df[df['Plate Type'] == 'COM']\n",
    "com_df = client.persist(com_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of Summons for Commercial Tickets 17.02794809010955\n"
     ]
    }
   ],
   "source": [
    "percent = 100 * len(com_df.index) / len(df.index) \n",
    "print(\"% of Summons for Commercial Tickets\", percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : Build a Cluster with Dask and Jupyter Lab using Helm (2 points)\n",
    "\n",
    "Your challenge is to build a Dask Cluster with 5 nodes (a scheduler, a server to deploy a jupyter\n",
    "notebook, and 3 workers) using Kubernetes on Google Cloud and Helm. Helm is a popular\n",
    "Kubernetes package manager currently maintained by the Cloud Native Computing Foundation\n",
    "(CNCF). Similar to Docker, Helm has hundreds of images (called charts) ready for deployment. We\n",
    "used Helm version 3.2.4 to test this part of the question. Original instructions to install dask on kubernetes via helm can be found [here](https://docs.dask.org/en/latest/setup/kubernetes-helm.html#kubernetes-helm-single).\n",
    "\n",
    "**Step 0:** Install [helm](https://helm.sh/docs/intro/install/) and add helm chart. Helm uses a packaging format called charts. A chart is a collection of files that describe a related set of Kubernetes resources.  `brew install helm` worked on mac for installing mac. To add helm chart  use `helm repo add dask https://helm.dask.org/` <br/> \n",
    "\n",
    "**Step 1:** Create a Google Cloud cluster with 5 nodes using the CLI to manage Kubernetes service.  <br/>\n",
    "`export PROJECT_ID=<your project id>` <br/>\n",
    "`gcloud config set project $PROJECT_ID` <br/>\n",
    "`gcloud config list` - This is to ensure you have right account, zone and project set.<br/> \n",
    "`gcloud container clusters create mydask-cluster --num-nodes 5` \n",
    "\n",
    "**Submit a screenshot** <br/>\n",
    " Do not change the number of nodes or default machine type when you create the cluster.\n",
    "\n",
    "\n",
    "**Step 2:** Use the `values.yaml` file provided to you. We have customized this file to add a few packages and loadbalancers, you can find the original [here](https://github.com/dask/helm-chart/tree/master/dask).  \n",
    "Use Helm to copy charts in each pod\n",
    "`helm install my-release -f values.yaml dask/dask`  <br/>  <br/>\n",
    "**Question :** Please compare the original and modified YAML files. What changes did we make to the load balancers? Why did we make them?\n",
    " \n",
    "<br/>\n",
    "\n",
    "\n",
    "**Step 3:** Check if all the pods are running and services are up (this may take few minutes) \n",
    "Submit a screenshot of `kubectl get pods` and `kubectl get services`\n",
    "        <br/>                                                                                                                 \n",
    "**Step 4:**Copy external-ip of dask-jupyter, access jupyter from your browser (password: dask) \n",
    "Also copy the external-ip of dask-scheduler and access the dask dashboard from your browser. \n",
    "   <br/> \n",
    "   \n",
    "**Step 5:** We have provided `helm_gcsfs.ipynb` for this part - this is the same exercise you have seen in lecture 4 but reads from the google cloud storage bucket. `gcsfs` is python based file-system interface to Google Cloud Storage. We have already installed `gcsfs` package on our cluster using `values.yaml`. First we will upload the dataset on Google Cloud Storage bucket and then run `helm_gcsfs.ipynb`. <br/>\n",
    "\n",
    "Now we are going to create a Google Cloud Storage Bucket (and then upload the dataset `Parking_Violations_Issued_-_Fiscal_Year_2017.csv`) - On cloud console search for `storage`. Mine looks like this - \n",
    "![Cloud storage](images/image10.png)\n",
    "(i) Create a bucket, give it a name and leave the defaults as is. Upload `Parking_Violations_Issued_-_Fiscal_Year_2017.csv` within this bucket. <br/> \n",
    "\n",
    "(ii) Once uploaded, click on the file and then click `Edit Permissions`. `Add Entry` - `Public` - `All Users` - `Reader`  and save.  <br/> Ideally we do not want to upload large datsets, we should mount the bucket and download the dataset from kaggle. For the purpose of this homework we will directly upload the file to google cloud storage bucket. **(Submit a screenshot)**\n",
    "Example: <br/>\n",
    "<br/>\n",
    "![Cloud Bucket](images/image11.png)\n",
    "**Question :** Why is this step necessary, why don't we just directly upload the .csv file on Jupyter lab ?\n",
    "<br/>\n",
    " \n",
    "(iii) Upload the attached `helm_gcsfs.ipynb` to jupyter lab.  In `helm_gcsfs.ipynb` change the name of your project and the name of the bucket. Run the code, download your `helm_gcsfs.ipynb` and submit along with this notebook. Also **submit 2-3 screenshot** of dask dashboard - we would like to see some computation happening, so take the screenshot while your program is running.    <br/>\n",
    "**Question :** How much time did helm_gcfs.ipynb took (time is reported in the notebook) ? <br/> \n",
    "\n",
    "\n",
    "Example screenshots: \n",
    "\n",
    "![Dask status](images/image13_dask_status.png)\n",
    "![Dask workers](images/image12_dask_workers.png)\n",
    "\n",
    "\n",
    "\n",
    "### <font color=red> Step 6: Delete your cluster </font>\n",
    "`gcloud container clusters delete mydask-cluster`\n",
    "\n",
    "Optional: Repeat step 1-6 with  6 workers - did it take same amount of time or less time ? Hint: change values.yaml to have 6 workers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun fact about user [jovyan](https://github.com/jupyter/docker-stacks/issues/358)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answers 1.5__\n",
    "1. ![](images/1.png)\n",
    "2. In order to load balance we set the service type to LoadBalancer to expose the outside of our cluster\n",
    "3. ![](images/2.png)\n",
    "5. ![](images/3.png) We upload this to google cloud storage because we want it to persist past our cluster, and also be accessible to all the workers in our cluster.\n",
    "6. ![](images/4.png)\n",
    "  ![](images/5.png)\n",
    " This took 50.88 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AC295]",
   "language": "python",
   "name": "conda-env-AC295-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
